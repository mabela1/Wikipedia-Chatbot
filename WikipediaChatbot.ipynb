{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtF6yZbZ89LR8fOf/hd2PQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "# To scrape Wikipedia\n",
        "from bs4 import BeautifulSoup\n",
        "# To access contents from URLs\n",
        "import requests\n",
        "# to preprocess text\n",
        "import nltk\n",
        "# to handle punctuations\n",
        "from string import punctuation\n",
        "# TF-IDF vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# cosine similarity score\n",
        "from sklearn.metrics.pairwise import cosine_similarity \n",
        "# to do array operations\n",
        "import numpy as np\n",
        "# to have sleep option\n",
        "from time import sleep "
      ],
      "metadata": {
        "id": "FnOtG7h3qTmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPhIh3iOnzt-",
        "outputId": "1f888562-9602-4f78-c979-360d8f02e550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatBot ...\n",
            "Shkruaj \"bye\" ose \"quit\" ose \"exit\" per te mbyllur biseden\n",
            "\n",
            "Shkruani temen e kerkuar.\n",
            "ChatBot do pergjigjet.         \n",
            "Nese shkruani \"more\",do jape me teper detaje         \n",
            "Nese shkruani \"jump\",do te kaloje ne info tjeter\n",
            "--------------------------------------------------\n",
            "ChatBot >>  Pershendetje,me thoni temen e interesit. \n",
            "User    >> Albania\n",
            "ChatBot >>  Tema ne \"Wikipedia: Albania\". Let's chat!\n",
            "User    >> Climate\n",
            "ChatBot >> Climate change is predicted to have serious effects on the living conditions in Albania.\n",
            "User    >> more\n",
            "ChatBot >> Environmental issues in Albania include air and water pollution , climate change , waste management , biodiversity loss and nature conservation . Climate change is predicted to have serious effects on the living conditions in Albania. The country is recognised as vulnerable to climate change impacts, ranked 80 among 181 countries in the Notre Dame Global Adaptation Index of 2019. Factors that account for the country's vulnerability to climate change risks include geological and hydrological hazards , including earthquakes, flooding, fires, landslides, torrential rains, river and coastal erosion. \n",
            "User    >> more\n",
            "ChatBot >> Environmental issues in Albania include air and water pollution , climate change , waste management , biodiversity loss and nature conservation . Climate change is predicted to have serious effects on the living conditions in Albania. The country is recognised as vulnerable to climate change impacts, ranked 80 among 181 countries in the Notre Dame Global Adaptation Index of 2019. Factors that account for the country's vulnerability to climate change risks include geological and hydrological hazards , including earthquakes, flooding, fires, landslides, torrential rains, river and coastal erosion. \n",
            "User    >> bye\n",
            "ChatBot >>  Shihemi se shpejti! Bye!\n",
            "\n",
            "Duke u mbyllur ChatBot ...\n"
          ]
        }
      ],
      "source": [
        "class ChatBot():\n",
        "    \n",
        "    # initialize bot\n",
        "    def __init__(self):\n",
        "        # flag whether to end chat\n",
        "        self.end_chat = False\n",
        "        # flag whether topic is found in wikipedia\n",
        "        self.got_topic = False\n",
        "        # flag whether to call respond()\n",
        "        # in some cases, response be made already\n",
        "        self.do_not_respond = True\n",
        "        \n",
        "        # wikipedia title\n",
        "        self.title = None\n",
        "        # wikipedia scraped data as paragraphs\n",
        "        self.text_data = []\n",
        "        # data as sentences\n",
        "        self.sentences = []\n",
        "        # to keep track of paragraph indices\n",
        "        # corresponding to all sentences\n",
        "        self.para_indices = []\n",
        "        # currently retrieved sentence id\n",
        "        self.current_sent_idx = None\n",
        "\t\t\n",
        "\t\t        \n",
        "        # a punctuation dictionary\n",
        "        self.punctuation_dict = str.maketrans({p:None for p in punctuation})\n",
        "        # wordnet lemmatizer for preprocessing text\n",
        "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "        # collection of stopwords\n",
        "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
        "        # initialize chatting\n",
        "        self.greeting()\n",
        "\n",
        "    # greeting method - to be called internally\n",
        "    # chatbot initializing chat on screen with greetings\n",
        "    def greeting(self):\n",
        "        print(\"ChatBot ...\")\n",
        "        # some time to get user ready\n",
        "        sleep(2)\n",
        "        # chat ending tags\n",
        "        print('Shkruaj \"bye\" ose \"quit\" ose \"exit\" per te mbyllur biseden')\n",
        "        sleep(2)\n",
        "        # chatbot descriptions\n",
        "        print('\\nShkruani temen e kerkuar.')\n",
        "        sleep(3)\n",
        "        print('ChatBot do pergjigjet. \\\n",
        "        \\nNese shkruani \"more\",do jape me teper detaje \\\n",
        "        \\nNese shkruani \"jump\",do te kaloje ne info tjeter')\n",
        "        # give time to read what has been printed\n",
        "        sleep(3)\n",
        "        print('-'*50)\n",
        "        # Greet and introduce\n",
        "        greet = \"Pershendetje,me thoni temen e interesit. \"\n",
        "        print(\"ChatBot >>  \" + greet)\n",
        "\t\t\n",
        "\n",
        "    def chat(self):\n",
        "        # continue chat\n",
        "        while not self.end_chat:\n",
        "            # receive input\n",
        "            self.receive_input()\n",
        "            # finish chat if opted by user\n",
        "            if self.end_chat:\n",
        "                print('ChatBot >>  Shihemi se shpejti! Bye!')\n",
        "                sleep(2)\n",
        "                print('\\nDuke u mbyllur ChatBot ...')\n",
        "            # if data scraping successful\n",
        "            elif self.got_topic:\n",
        "                # in case not already responded\n",
        "                if not self.do_not_respond:\n",
        "                    self.respond()\n",
        "                # clear flag so that bot can respond next time\n",
        "                self.do_not_respond = False\n",
        "\t\t\t\t\n",
        "\t# receive_input method - to be called internally\n",
        "    # recieves input from user and makes preliminary decisions\n",
        "    def receive_input(self):\n",
        "        # receive input from user\n",
        "        text = input(\"User    >> \")\n",
        "        # end conversation if user wishes so\n",
        "        if text.lower().strip() in ['bye', 'quit', 'exit']:\n",
        "            # turn flag on \n",
        "            self.end_chat=True\n",
        "        # if user needs more information \n",
        "        elif text.lower().strip() == 'more':\n",
        "            # respond here itself\n",
        "            self.do_not_respond = True\n",
        "            # if at least one query has been received \n",
        "            if self.current_sent_idx != None:\n",
        "                response = self.text_data[self.para_indices[self.current_sent_idx]]\n",
        "            # prompt user to start querying\n",
        "            else:\n",
        "                response = \"Shkruaj input-in tuaj!\"\n",
        "            print(\"ChatBot >> \" + response)\n",
        "        # if topic is not chosen\n",
        "        elif not self.got_topic:\n",
        "            self.scrape_wiki(text)\n",
        "        else:\n",
        "            # add user input to sentences, so that we can vectorize in whole\n",
        "            self.sentences.append(text)\n",
        "\t\n",
        "\t# respond method - to be called internally\n",
        "    def respond(self):\n",
        "        # tf-idf-modeling\n",
        "        vectorizer = TfidfVectorizer(tokenizer=self.preprocess)\n",
        "        # fit data and obtain tf-idf vector\n",
        "        tfidf = vectorizer.fit_transform(self.sentences)\n",
        "        # calculate cosine similarity scores\n",
        "        scores = cosine_similarity(tfidf[-1],tfidf) \n",
        "        # identify the most closest sentence\n",
        "        self.current_sent_idx = scores.argsort()[0][-2]\n",
        "        # find the corresponding score value\n",
        "        scores = scores.flatten()\n",
        "        scores.sort()\n",
        "        value = scores[-2]\n",
        "        # if there is matching sentence\n",
        "        if value != 0:\n",
        "            print(\"ChatBot >> \" + self.sentences[self.current_sent_idx]) \n",
        "        # if no sentence is matching the query\n",
        "        else:\n",
        "            print(\"ChatBot >>  Shkruaj!\" )\n",
        "        # remove the user query from sentences\n",
        "        del self.sentences[-1]\n",
        "\t\t\n",
        "\t        \n",
        "    # scrape_wiki method - to be called internally.\n",
        "    # called when user inputs topic of interest.\n",
        "    # employs requests to access Wikipedia via URL.\n",
        "    # employs BeautifulSoup to scrape paragraph tagged data\n",
        "    # and h1 tagged article heading.\n",
        "    # employs NLTK to tokenize data\n",
        "    def scrape_wiki(self,topic):\n",
        "        # process topic as required by Wikipedia URL system\n",
        "        topic = topic.lower().strip().capitalize().split(' ')\n",
        "        topic = '_'.join(topic)\n",
        "        try:\n",
        "            # creata an url\n",
        "            link = 'https://en.wikipedia.org/wiki/'+ topic\n",
        "            # access contents via url\n",
        "            data = requests.get(link).content\n",
        "            # parse data as soup object\n",
        "            soup = BeautifulSoup(data, 'html.parser')\n",
        "            # extract all paragraph data\n",
        "            # scrape strings with html tag 'p'\n",
        "            p_data = soup.findAll('p')\n",
        "            # scrape strings with html tag 'dd'\n",
        "            dd_data = soup.findAll('dd')\n",
        "            # scrape strings with html tag 'li'\n",
        "            #li_data = soup.findAll('li')\n",
        "            p_list = [p for p in p_data]\n",
        "            dd_list = [dd for dd in dd_data]\n",
        "            #li_list = [li for li in li_data]\n",
        "            # iterate over all data\n",
        "            for tag in p_list+dd_list: #+li_list:\n",
        "                # a bucket to collect processed data\n",
        "                a = []\n",
        "                # iterate over para, desc data and list items contents\n",
        "                for i in tag.contents:\n",
        "                    # exclude references, superscripts, formattings\n",
        "                    if i.name != 'sup' and i.string != None:\n",
        "                        stripped = ' '.join(i.string.strip().split())\n",
        "                        # collect data pieces\n",
        "                        a.append(stripped)\n",
        "                # with collected string pieces formulate a single string\n",
        "                # each string is a paragraph\n",
        "                self.text_data.append(' '.join(a))\n",
        "            \n",
        "\t\t\t\t\n",
        "            # obtain sentences from paragraphs\n",
        "            for i,para in enumerate(self.text_data):\n",
        "                sentences = nltk.sent_tokenize(para)\n",
        "                self.sentences.extend(sentences)\n",
        "                # for each sentence, its para index must be known\n",
        "                # it will be useful in case user prompts \"more\" info\n",
        "                index = [i]*len(sentences)\n",
        "                self.para_indices.extend(index)\n",
        "            \n",
        "            # extract h1 heading tag from soup object\n",
        "            self.title = soup.find('h1').string\n",
        "            # turn respective flag on\n",
        "            self.got_topic = True\n",
        "            # announce user that chatbot is ready now\n",
        "            print('ChatBot >>  Tema ne \"Wikipedia: {}\". Let\\'s chat!'.format(self.title)) \n",
        "        # in case of unavailable topics\n",
        "        except Exception as e:\n",
        "            print('ChatBot >>  Error: {}. \\\n",
        "            Vendosni nje teme tjeter!'.format(e))\n",
        "        \n",
        "\t\t\n",
        "    # preprocess method - to be called internally by Tf-Idf vectorizer\n",
        "    # text preprocessing, stopword removal, lemmatization, word tokenization\n",
        "    def preprocess(self, text):\n",
        "        # remove punctuations\n",
        "        text = text.lower().strip().translate(self.punctuation_dict) \n",
        "        # tokenize into words\n",
        "        words = nltk.word_tokenize(text) \n",
        "        # remove stopwords\n",
        "        words = [w for w in words if w not in self.stopwords]\n",
        "        # lemmatize \n",
        "        return [self.lemmatizer.lemmatize(w) for w in words]\n",
        "\t\t\n",
        "\n",
        "# Happy Chatting!\n",
        "# Initialize ChatBot and start chatting.\n",
        "\t\t\n",
        "if __name__ == '__main__':\n",
        "\t# instantiate an object\n",
        "\twiki = ChatBot()\n",
        "\t# call chat method\n",
        "\twiki.chat()\n",
        "# end of script"
      ]
    }
  ]
}